{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [recent study](https://pubmed.ncbi.nlm.nih.gov/27400279/) from the [Segata Lab](http://segatalab.cibio.unitn.it/) analyzes human microbiome data obtained using shotgun metagenomic analysis.\n",
    "\n",
    "\"Shotgun metagenomic analysis of the human associated microbiome provides a rich set of microbial features for prediction and biomarker discovery in the context of human diseases and health conditions. However, the use of such high-resolution microbial features presents new challenges, and validated computational tools for learning tasks are lacking.\"\n",
    "\n",
    "The dataset used in the study is available freely at [MetAML - Metagenomic prediction Analysis based on Machine Learning](https://github.com/SegataLab/metaml) and on [kaggle](https://www.kaggle.com/antaresnyc/metagenomics). \n",
    "\n",
    "The dataset gives abundances of different microbial species present in microbiome samples from healthy people and patients  with a variety of different diseases. The goal of the above mentioned study was to correctly predict whether a patient had a given disease or was healthy from the species abundance data for the patient's microbiome sample, after training on healthy samples and samples from patients with the given disease. \n",
    "\n",
    "In a practical setting, one may not have access to many samples from patients with a given disease and would want to be able to predict from a patient's microbiome data whether they have one of two possible diseases given only a few labeled examples for each of the two diseases. This problem belongs to a class of problems known as \"Few-shot learning\" studied in the Machine Learning Literature.\n",
    "\n",
    "Our goal will be train a model to classify a given microbiome sample as having one of two previously unseen diseases (not present in the training set), from only a few labeled examples for each of the two diseases. I use a recent elegant and general algorithm known as [Model Agnostic Meta-Learning algorithm](https://arxiv.org/abs/1703.03400) to learn to classify the species abundance data into two unseen disease classes, given only 3 labeled examples, i.e. the 3-shot 2-way classification task. \n",
    "\n",
    "In the following, I classify species abundance samples into 'Obesity' vs 'Type 2 Diabetes' given only 3 labeled examples for each, and by training on the data for the remaining diseases. Guided by the performance comparison of different model architechtures on a similar task [studied here](https://www.nature.com/articles/s41598-019-46649-z), I use a single hidden layer Multi-Layer Perceptron with 128 neurons.\n",
    "\n",
    "The model achieves an impressive performance comparable to the best previous results on this dataset, with an accuracy (> 80 %), an F1 score (> 0.8), an ROC AUC score (> 0.8), and a Confusion Matrix ~((8x, x),(x, 8x)).\n",
    "\n",
    "It is worth noting that the validation and training data in this case have no disease classes in common, and that both are balanced, i.e. have equal numbers of samples for each disease.\n",
    "\n",
    "Different seed choices lead to different choices of diseases for testing. \n",
    "\n",
    "I build on the previous notebooks using this [metagenomics dataset](https://www.kaggle.com/antaresnyc/metagenomics): \n",
    "* [sklasfeld/starter-metagenomics](https://www.kaggle.com/sklasfeld/starter-metagenomics)\n",
    "* [antaresnyc/cirrhosis-classification/notebook](https://www.kaggle.com/antaresnyc/cirrhosis-classification/notebook)\n",
    "* [tmathieu/feature-selection-to-predict-diabetes](https://www.kaggle.com/tmathieu/feature-selection-to-predict-diabetes)\n",
    "\n",
    "I closely follow the excellent [MAML implementation by Oscar Knagg](https://github.com/oscarknagg/few-shot) and the Perceptron implementation [described here](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/1_mlp.ipynb#scrollTo=C6gprB1sO7fy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-19T19:57:28.618245Z",
     "iopub.status.busy": "2022-01-19T19:57:28.617449Z",
     "iopub.status.idle": "2022-01-19T19:57:29.950767Z",
     "shell.execute_reply": "2022-01-19T19:57:29.950018Z",
     "shell.execute_reply.started": "2022-01-19T19:57:28.618145Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/metagenomics/markers2clades_DB.txt\n",
      "/kaggle/input/metagenomics/abundance_stoolsubset.txt\n",
      "/kaggle/input/metagenomics/abundance.txt\n",
      "/kaggle/input/metagenomics/marker_presence.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from typing import List, Iterable, Callable, Tuple\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve, recall_score\n",
    "\n",
    "# import seaborn as sns\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Callable, Union\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "\n",
    "SEED = 1729\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data and preparing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T19:57:29.952895Z",
     "iopub.status.busy": "2022-01-19T19:57:29.952633Z",
     "iopub.status.idle": "2022-01-19T19:57:34.318468Z",
     "shell.execute_reply": "2022-01-19T19:57:34.317631Z",
     "shell.execute_reply.started": "2022-01-19T19:57:29.952859Z"
    }
   },
   "outputs": [],
   "source": [
    "def  file_to_dataframe(file):\n",
    "    # this file is transposed of what we expect in a pandas dataframe\n",
    "    # therefore we import the file and transpose it\n",
    "    df = pd.read_csv(file,sep=\"\\t\",dtype=object,header=None).T\n",
    "\n",
    "    # now we can set the header\n",
    "    header = df.iloc[0] #grab the first row for the header\n",
    "    df = df[1:] #take the data less the header row\n",
    "    df.columns = header #set the header row as the df header\n",
    "\n",
    "    return(df)\n",
    "\n",
    "abundance_full_df = file_to_dataframe(\"/kaggle/input/metagenomics/abundance.txt\")\n",
    "abundance_stool_full_df = file_to_dataframe(\n",
    "    \"/kaggle/input/metagenomics/abundance_stoolsubset.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following verifies that the file abundance_stool_subset.txt contains a subset of the data in abundance.txt, as asserted in the [starter metagenomics notebook](https://www.kaggle.com/sklasfeld/starter-metagenomics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-01-19T19:57:34.320447Z",
     "iopub.status.busy": "2022-01-19T19:57:34.320156Z",
     "iopub.status.idle": "2022-01-19T19:57:38.937786Z",
     "shell.execute_reply": "2022-01-19T19:57:38.936939Z",
     "shell.execute_reply.started": "2022-01-19T19:57:34.320407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That the data frames are the same is: True\n"
     ]
    }
   ],
   "source": [
    "abundance_subset_stool_cols_df = abundance_full_df.loc[\n",
    "                                abundance_full_df.bodysite == 'stool']\n",
    "stool_cols = abundance_stool_full_df.columns.to_numpy()\n",
    "abundance_subset_stool_cols_df = abundance_subset_stool_cols_df[stool_cols]\n",
    "unique_sample_ids = abundance_subset_stool_cols_df['sampleID'].unique()\n",
    "unique_subject_ids = abundance_subset_stool_cols_df['subjectID'].unique() \n",
    "num_cols = abundance_stool_full_df.shape[1]\n",
    "same_data = True\n",
    "for sample_id in unique_sample_ids:\n",
    "    abundance_stool_full_df_sample = abundance_stool_full_df.loc[\n",
    "                                    abundance_stool_full_df.sampleID == sample_id]\n",
    "    abundance_subset_stool_df_sample = abundance_subset_stool_cols_df.loc[\n",
    "                            abundance_subset_stool_cols_df.sampleID == sample_id]\n",
    "    unique_subject_ids = abundance_stool_full_df_sample['subjectID'].unique()\n",
    "    for subject_id in unique_subject_ids:\n",
    "        abundance_stool_full_df_data = abundance_stool_full_df_sample.loc[\n",
    "                        abundance_stool_full_df_sample.subjectID == subject_id]\n",
    "        abundance_subset_stool_data = abundance_subset_stool_df_sample.loc[\n",
    "                        abundance_subset_stool_df_sample.subjectID == subject_id]   \n",
    "        abundance_subset_stool_data = abundance_subset_stool_data.to_numpy()\n",
    "        abundance_stool_full_df_data = abundance_stool_full_df_data.to_numpy()\n",
    "        num_rows = abundance_subset_stool_data.shape[0]\n",
    "        if num_rows == 1:\n",
    "            comparison_result = np.all(abundance_stool_full_df_data ==\n",
    "                                abundance_subset_stool_data)\n",
    "            if not comparison_result:        \n",
    "                same_data = False\n",
    "                print('different data')\n",
    "                print(abundance_stool_full_df_data != abundance_subset_stool_data)\n",
    "        else:\n",
    "            is_unequal = 1.\n",
    "            for i in range(num_rows):\n",
    "                x_row = abundance_subset_stool_data[i]\n",
    "                for j in range(num_rows):\n",
    "                    y_row = abundance_stool_full_df_data[j]\n",
    "                    comparison_result = np.all(x_row == y_row)\n",
    "                    if comparison_result: \n",
    "                        is_unequal = is_unequal*0.\n",
    "                    else:\n",
    "                        is_unequal = is_unequal*1.                        \n",
    "            if is_unequal:\n",
    "                same_data = False\n",
    "                print('unequal dataframes')\n",
    "                print(abundance_subset_stool_data != abundance_stool_full_df_data)\n",
    "\n",
    "print(f'That the data frames are the same is: {same_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T19:57:38.940533Z",
     "iopub.status.busy": "2022-01-19T19:57:38.939812Z",
     "iopub.status.idle": "2022-01-19T19:57:39.118024Z",
     "shell.execute_reply": "2022-01-19T19:57:39.117246Z",
     "shell.execute_reply.started": "2022-01-19T19:57:38.940491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of observations for each disease:\n",
      "{'n': 2054, 'obesity': 164, 'leaness': 89, 'nd': 475, 'stec2-positive': 52, ' -': 20, 'ibd_ulcerative_colitis': 148, 'ibd_crohn_disease': 25, 'n_relative': 47, 'y': 36, 'cirrhosis': 118, 'obese': 5, 'overweight': 10, '-': 7, 'underweight': 1, 't2d': 223, 'impaired_glucose_tolerance': 49, 'cancer': 48, 'small_adenoma': 26, 'large_adenoma': 13}\n",
      "\n",
      " diseases with less 60 samples:\n",
      "['stec2-positive', ' -', 'ibd_crohn_disease', 'n_relative', 'y', 'obese', 'overweight', '-', 'underweight', 'impaired_glucose_tolerance', 'cancer', 'small_adenoma', 'large_adenoma']\n"
     ]
    }
   ],
   "source": [
    "#### Printing the diseases and the number of observations for each disease ####\n",
    "\n",
    "disease_list = abundance_full_df.loc[:,'disease'].unique()\n",
    "\n",
    "num_obs_list = [abundance_full_df.loc[abundance_full_df.disease == disease_list[i]\n",
    "                                      ].shape[0] for i in range(len(disease_list))]\n",
    "disease_obs_dict = {disease_list[i]:num_obs_list[i] for i in range(len(\n",
    "                                                            disease_list))}\n",
    "print(f\"# of observations for each disease:\")\n",
    "print(disease_obs_dict)\n",
    "low_data_diseases = [d for d in disease_list if disease_obs_dict[d] < 60]\n",
    "print('\\n diseases with less 60 samples:')\n",
    "print(low_data_diseases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will merge 'n', 'nd' and 'leaness' into the label 'control', and merge the two variants of IBD into the label 'ibd'. We will retain only the data corresponding to the disease labels 'ibd', 'stec2-positive', 'impaired_glucose_tolerance', 'cancer', 'cirrhosis', 'obesity', 't2d', and 'control', and get rid of the rest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T19:57:39.119434Z",
     "iopub.status.busy": "2022-01-19T19:57:39.119174Z",
     "iopub.status.idle": "2022-01-19T19:57:39.394549Z",
     "shell.execute_reply": "2022-01-19T19:57:39.393783Z",
     "shell.execute_reply.started": "2022-01-19T19:57:39.119396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataframe shape is (3610, 3513)\n",
      "selected dataframe shape is (3445, 3513)\n",
      "number of diseases is 8 :\n",
      "# of observations for each disease:\n",
      "{'control': 2618, 'obesity': 164, 'stec2-positive': 52, 'ibd': 173, 'cirrhosis': 118, 't2d': 223, 'impaired_glucose_tolerance': 49, 'cancer': 48}\n"
     ]
    }
   ],
   "source": [
    "### merge 'n', 'nd', 'leaness' into 'control'\n",
    "abundance_full_df.loc[:,'disease'] = abundance_full_df['disease'].apply(\n",
    "    lambda x: 'control' if ((x == 'n') or (x == 'nd') or (x == 'leaness')) else x)\n",
    "\n",
    "### merge 'ibd_crohn_disease' and 'ibd_ulcerative_colitis' into 'ibd'.\n",
    "abundance_full_df.loc[:,'disease'] = abundance_full_df['disease'].apply(\n",
    "    lambda x: 'ibd' if ('ibd' in x) else x)\n",
    "\n",
    "### retaining data only for the following diseases\n",
    "diseases = ['control', 'obesity', 'ibd', 'stec2-positive', \n",
    "            'impaired_glucose_tolerance', 'cirrhosis', 't2d', 'cancer']\n",
    "abundance_df = abundance_full_df.loc[\n",
    "                            abundance_full_df['disease'].isin(diseases)]\n",
    "\n",
    "print(f'original dataframe shape is {abundance_full_df.shape}')\n",
    "print(f'selected dataframe shape is {abundance_df.shape}')\n",
    "disease_list = abundance_df['disease'].unique()\n",
    "print(f\"number of diseases is {disease_list.size} :\")\n",
    "\n",
    "num_obs_list = [abundance_df.loc[abundance_df.disease == disease_list[i]\n",
    "                                      ].shape[0] for i in range(len(disease_list))]\n",
    "disease_obs_dict = {disease_list[i]:num_obs_list[i] for i in range(len(\n",
    "                                                            disease_list))}\n",
    "print(f\"# of observations for each disease:\")\n",
    "print(disease_obs_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the steps in the [cirrhosis classification notebook](https://www.kaggle.com/antaresnyc/cirrhosis-classification/notebook) to convert the species abundance column values from strings to floats, and add a disease_id column which stores a unique integer value for each disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T19:57:39.396200Z",
     "iopub.status.busy": "2022-01-19T19:57:39.395862Z",
     "iopub.status.idle": "2022-01-19T19:57:41.302346Z",
     "shell.execute_reply": "2022-01-19T19:57:41.301602Z",
     "shell.execute_reply.started": "2022-01-19T19:57:39.396161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "associating an id with each disease:\n",
      "{'control': 0, 'obesity': 1, 'ibd': 2, 'stec2-positive': 3, 'impaired_glucose_tolerance': 4, 'cirrhosis': 5, 't2d': 6, 'cancer': 7}\n",
      "number of species is 3302\n",
      "number of metadata columns is 212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "disease_index_dict = {d:diseases.index(d) for d in diseases}\n",
    "print(f'associating an id with each disease:')\n",
    "print(disease_index_dict)\n",
    "abundance_df['disease_id'] = abundance_df['disease'].apply(\n",
    "                                    lambda x: diseases.index(x))\n",
    "\n",
    "cols = abundance_df.columns.tolist()\n",
    "species = [x for x in cols if x.startswith('k_')]\n",
    "print(f'number of species is {len(species)}')\n",
    "metadata = [x for x in cols if not x.startswith('k_')]\n",
    "print(f'number of metadata columns is {len(metadata)}')\n",
    "\n",
    "species_df = abundance_df.loc[:,species].copy()\n",
    "species_df = species_df.astype('float32')\n",
    "abundance_df = pd.concat([abundance_df.loc[:,metadata], species_df], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of different disease labels is 8. We will split the dataset into a training and test dataset, such that the disease labels in the test data are not present in the training data. We will choose the disease labels for the test data to be 2 diseases randomly chosen from among all the diseases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T19:57:41.304157Z",
     "iopub.status.busy": "2022-01-19T19:57:41.303886Z",
     "iopub.status.idle": "2022-01-19T19:57:41.345020Z",
     "shell.execute_reply": "2022-01-19T19:57:41.344094Z",
     "shell.execute_reply.started": "2022-01-19T19:57:41.304122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 1]\n",
      "[0, 2, 3, 4, 5, 7]\n",
      "[0 3 2 5 4 7]\n",
      "[1 6]\n"
     ]
    }
   ],
   "source": [
    "disease_ids = np.arange(1, len(diseases)).tolist()\n",
    "test_diseases = random.sample(disease_ids, 2)\n",
    "print(test_diseases)\n",
    "train_diseases = [diseases.index(d) \n",
    "                  for d in diseases if diseases.index(d) not in test_diseases]\n",
    "print(train_diseases)\n",
    "\n",
    "train_df = abundance_df.loc[abundance_df['disease_id'].isin(\n",
    "                                train_diseases)]\n",
    "print(train_df['disease_id'].unique())\n",
    "test_df = abundance_df.loc[abundance_df['disease_id'].isin(\n",
    "                                test_diseases)]\n",
    "print(test_df['disease_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use feature selection on the species columns to select species whose abundance shows high variance across different diseases, as in the [original paper](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004977) and in [this notebook](https://www.kaggle.com/tmathieu/feature-selection-to-predict-diabetes), albeit with a broader criterion.\n",
    "\n",
    "We will only consider the data corresponding to different disease classes and neglect the data corresponding to the 'control' label for feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T19:57:41.346866Z",
     "iopub.status.busy": "2022-01-19T19:57:41.346556Z",
     "iopub.status.idle": "2022-01-19T19:57:41.616014Z",
     "shell.execute_reply": "2022-01-19T19:57:41.615344Z",
     "shell.execute_reply.started": "2022-01-19T19:57:41.346830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train df has the following diseases:\n",
      "['stec2-positive' 'ibd' 'cirrhosis' 'impaired_glucose_tolerance' 'cancer']\n",
      "number of total species are: 3302\n",
      "number of selected species are: 255\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "diseased_train_df = train_df.loc[train_df['disease_id'] != diseases.index('control')]\n",
    "print('train df has the following diseases:')\n",
    "print(diseased_train_df.disease.unique())\n",
    "counts, bins, histplot = plt.hist(diseased_train_df[species].std(), bins=20)\n",
    "select_species = np.array(species)[diseased_train_df[species].std().values>1.]\n",
    "print(f\"number of total species are: {len(species)}\")\n",
    "print(f\"number of selected species are: {select_species.size}\")\n",
    "\n",
    "select_species_id = np.append(select_species, ['disease_id'])\n",
    "train_df = train_df.loc[:,select_species_id]\n",
    "train_df['id'] = np.arange(train_df.shape[0])\n",
    "test_df = test_df.loc[:,select_species_id]\n",
    "test_df['id'] = np.arange(test_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing MAML with a multi-layer perceptron\n",
    "\n",
    "We will now use few-shot learning to learn to predict the labels for these diseases. Our aim will be to distinguish a diseased microbiome sample for any given disease from a healthy, i.e. control sample given only a few labeled examples. \n",
    "\n",
    "We will use the technique of [Model Agnostic Meta-Learning by Finn et al](https://arxiv.org/abs/1703.03400) to learn to classify new diseases given only a few examples and closely follow the excellent [MAML implementation by Oscar Knagg](https://github.com/oscarknagg/few-shot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T19:57:41.617784Z",
     "iopub.status.busy": "2022-01-19T19:57:41.617509Z",
     "iopub.status.idle": "2022-01-19T19:57:41.636438Z",
     "shell.execute_reply": "2022-01-19T19:57:41.635472Z",
     "shell.execute_reply.started": "2022-01-19T19:57:41.617748Z"
    }
   },
   "outputs": [],
   "source": [
    "### taken from Oscar Knagg's implementation ###\n",
    "\n",
    "class NShotTaskSampler(data.Sampler):\n",
    "    def __init__(self,\n",
    "                 dataset: torch.utils.data.Dataset,\n",
    "                 episodes_per_epoch: int = None,\n",
    "                 n: int = None,\n",
    "                 k: int = None,\n",
    "                 q: int = None,\n",
    "                 num_tasks: int = 1):\n",
    "        \"\"\"PyTorch Sampler subclass that generates batches of n-shot, k-way, \n",
    "        q-query tasks.\n",
    "\n",
    "        Each n-shot task contains a \"support set\" of `k` sets of `n` samples and \n",
    "        a \"query set\" of `k` sets of `q` samples. The support set and the query set \n",
    "        are all grouped into one Tensor such that the first n * k samples are from \n",
    "        the support set while the remaining q * k samples are from the query set.\n",
    "\n",
    "        The support and query sets are sampled such that they are disjoint \n",
    "        i.e. do not contain overlapping samples.\n",
    "\n",
    "        # Arguments\n",
    "            dataset: Instance of torch.utils.data.Dataset from which to draw samples\n",
    "            episodes_per_epoch: Arbitrary number of batches of n-shot tasks to \n",
    "                                generate in one epoch\n",
    "            n_shot: int. Number of samples for each class in the n-shot \n",
    "                            classification tasks.\n",
    "            k_way: int. Number of classes in the n-shot classification tasks.\n",
    "            q_queries: int. Number query samples for each class in the n-shot \n",
    "                            classification tasks.\n",
    "            num_tasks: Number of n-shot tasks to group into a single batch\n",
    "        \"\"\"\n",
    "        super(NShotTaskSampler, self).__init__(dataset)\n",
    "        self.episodes_per_epoch = episodes_per_epoch\n",
    "        self.dataset = dataset\n",
    "        if num_tasks < 1:\n",
    "            raise ValueError('num_tasks must be > 1.')\n",
    "\n",
    "        self.num_tasks = num_tasks\n",
    "        # TODO: Raise errors if initialise badly\n",
    "        self.k = k\n",
    "        self.n = n\n",
    "        self.q = q\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.episodes_per_epoch\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.episodes_per_epoch):\n",
    "            batch = []\n",
    "\n",
    "            for task in range(self.num_tasks):\n",
    "                # Get random classes\n",
    "                episode_classes = np.random.choice(\n",
    "                    self.dataset.data_frame['disease_id'].unique(), size=self.k, \n",
    "                                            replace=False)\n",
    "\n",
    "                df = self.dataset.data_frame[\n",
    "                        self.dataset.data_frame['disease_id'].isin(episode_classes)]\n",
    "\n",
    "                support_k = {k: None for k in episode_classes}\n",
    "                for k in episode_classes:\n",
    "                    # Select support examples\n",
    "                    support = df[df['disease_id'] == k].sample(self.n)\n",
    "                    support_k[k] = support\n",
    "\n",
    "                    for i, s in support.iterrows():\n",
    "                        batch.append(s['id'])\n",
    "\n",
    "                for k in episode_classes:\n",
    "                    query = df[(df['disease_id'] == k) & (\n",
    "                        ~df['id'].isin(support_k[k]['id']))].sample(self.q)\n",
    "                    for i, q in query.iterrows():\n",
    "                        batch.append(q['id'])\n",
    "\n",
    "            yield np.stack(batch)\n",
    "            \n",
    "\n",
    "class SpeciesAbundanceDataset(data.Dataset):\n",
    "    \"\"\"Species Abundance dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, abundance_df, species_columns, target_column):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        \"\"\"\n",
    "        self.data_frame = abundance_df\n",
    "        self.species_cols = species_columns\n",
    "        self.target_col = target_column\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            target = self.data_frame[self.target_col].iloc[idx].to_numpy(dtype='int')\n",
    "        elif type(idx) != int:\n",
    "            idx = int(idx)\n",
    "            target = self.data_frame[self.target_col].iloc[idx]\n",
    "\n",
    "        species_abundance = self.data_frame[self.species_cols].iloc[idx].to_numpy(\n",
    "                                                                    dtype='float32')\n",
    "        sample = (species_abundance, target)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a multilayer perceptron to classify diseases from the microbiome metagenomics data, using [this notebook](https://colab.research.google.com/github/bentrevett/pytorch-image-classification/blob/master/1_mlp.ipynb#scrollTo=CHadwpvmREvE) as our guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T19:57:41.640550Z",
     "iopub.status.busy": "2022-01-19T19:57:41.639901Z",
     "iopub.status.idle": "2022-01-19T19:57:41.651913Z",
     "shell.execute_reply": "2022-01-19T19:57:41.651112Z",
     "shell.execute_reply.started": "2022-01-19T19:57:41.640508Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, layer_size: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_fc = nn.Linear(input_dim, layer_size)\n",
    "        self.output_fc = nn.Linear(layer_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        h_1 = F.relu(self.input_fc(x))\n",
    "        y_pred = self.output_fc(h_1)\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def functional_forward(self, x, weights):\n",
    "        \"\"\"Applies the same forward pass using PyTorch functional \n",
    "        operators using a specified set of weights.\"\"\"\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = F.linear(x, weights['input_fc.weight'], weights['input_fc.bias'])\n",
    "        x = F.relu(x)\n",
    "        x = F.linear(x, weights['output_fc.weight'], weights['output_fc.bias'])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write the Model Agnostic Meta-Learning gradient step function, closely following [this implementation](https://github.com/oscarknagg/few-shot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T19:57:41.655452Z",
     "iopub.status.busy": "2022-01-19T19:57:41.654973Z",
     "iopub.status.idle": "2022-01-19T19:57:41.681554Z",
     "shell.execute_reply": "2022-01-19T19:57:41.680613Z",
     "shell.execute_reply.started": "2022-01-19T19:57:41.655410Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_nshot_task_label(k: int, q: int) -> torch.Tensor:\n",
    "    \"\"\"Creates an n-shot task label.\n",
    "\n",
    "    Label has the structure:\n",
    "        [0]*q + [1]*q + ... + [k-1]*q\n",
    "\n",
    "    # Arguments\n",
    "        k: Number of classes in the n-shot classification task\n",
    "        q: Number of query samples for each class in the n-shot classification task\n",
    "\n",
    "    # Returns\n",
    "        y: Label vector for n-shot task of shape [q * k, ]\n",
    "    \"\"\"\n",
    "    y = torch.arange(0, k, 1 / q).long()\n",
    "\n",
    "    return y\n",
    "\n",
    "def prepare_meta_batch(n, k, q, meta_batch_size):\n",
    "    def prepare_meta_batch_(batch):\n",
    "        x, y = batch\n",
    "        # Reshape to `meta_batch_size` number of tasks. Each task contains\n",
    "        # n*k support samples to train the fast model on and q*k query samples to\n",
    "        # evaluate the fast model on and generate meta-gradients\n",
    "        x = x.reshape(meta_batch_size, n*k + q*k, x.shape[-1])\n",
    "        # Move to device\n",
    "        x = x.double().to(device)\n",
    "        # Create label\n",
    "        y = create_nshot_task_label(k, q).to(device).repeat(meta_batch_size)\n",
    "        return x, y\n",
    "\n",
    "    return prepare_meta_batch_\n",
    "\n",
    "\n",
    "def replace_grad(parameter_gradients, parameter_name):\n",
    "    def replace_grad_(module):\n",
    "        return parameter_gradients[parameter_name]\n",
    "\n",
    "    return replace_grad_\n",
    "\n",
    "def meta_gradient_step(model: nn.Module,\n",
    "                       optimizer: optim.Optimizer,\n",
    "                       loss_fn: Callable,\n",
    "                       x: torch.Tensor,\n",
    "                       y: torch.Tensor,\n",
    "                       n_shot: int,\n",
    "                       k_way: int,\n",
    "                       q_queries: int,\n",
    "                       order: int,\n",
    "                       inner_train_steps: int,\n",
    "                       inner_lr: float,\n",
    "                       train: bool,\n",
    "                       device: Union[str, torch.device]):\n",
    "    \"\"\"\n",
    "    Perform a gradient step on a meta-learner.\n",
    "\n",
    "    # Arguments\n",
    "        model: Base model of the meta-learner being trained\n",
    "        optimizer: Optimizer to calculate gradient step from loss\n",
    "        loss_fn: Loss function to calculate between predictions and outputs\n",
    "        x: Input samples for all few shot tasks\n",
    "        y: Input labels of all few shot tasks\n",
    "        n_shot: Number of examples per class in the support set of each task\n",
    "        k_way: Number of classes in the few shot classification task of each task\n",
    "        q_queries: Number of examples per class in the query set of each task.\n",
    "        The query set is used to calculate\n",
    "            meta-gradients after applying the update to\n",
    "        order: Whether to use 1st order MAML \n",
    "        (update meta-learner weights with gradients of the updated weights on the\n",
    "            query set) or 2nd order MAML (use 2nd order updates by differentiating \n",
    "            through the gradients of the updated weights on the query with respect \n",
    "            to the original weights).\n",
    "        inner_train_steps: Number of gradient steps to fit the fast weights \n",
    "                            during each inner update\n",
    "        inner_lr: Learning rate used to update the fast weights on the inner update\n",
    "        train: Whether to update the meta-learner weights at the end of the episode.\n",
    "        device: Device on which to run computation\n",
    "    \"\"\"\n",
    "    data_shape = x.shape[2:]\n",
    "    create_graph = (True if order == 2 else False) and train\n",
    "\n",
    "    task_gradients = []\n",
    "    task_losses = []\n",
    "    task_predictions = []\n",
    "    for meta_batch in x:\n",
    "        # By construction x is a 5D tensor of shape: \n",
    "        # (meta_batch_size, n*k + q*k, channels, width, height)\n",
    "        # Hence when we iterate over the first  dimension \n",
    "        # we are iterating through the meta batches\n",
    "        x_task_train = meta_batch[:n_shot * k_way]\n",
    "        x_task_val = meta_batch[n_shot * k_way:]\n",
    "\n",
    "        # Create a fast model using the current meta model weights\n",
    "        fast_weights = OrderedDict(model.named_parameters())\n",
    "\n",
    "        # Train the model for `inner_train_steps` iterations\n",
    "        for inner_batch in range(inner_train_steps):\n",
    "            # Perform update of model weights\n",
    "            y = create_nshot_task_label(k_way,n_shot).to(device)\n",
    "            y_pred = model.functional_forward(x_task_train, fast_weights)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            gradients = torch.autograd.grad(loss, fast_weights.values(), \n",
    "                                            create_graph=create_graph)\n",
    "\n",
    "            # Update weights manually\n",
    "            fast_weights = OrderedDict(\n",
    "                (name, param - inner_lr * grad)\n",
    "                for ((name, param), grad) in zip(fast_weights.items(), gradients))\n",
    "\n",
    "        # Do a pass of the model on the validation data from the current task\n",
    "        y = create_nshot_task_label(k_way,q_queries).to(device)\n",
    "        y_pred = model.functional_forward(x_task_val, fast_weights)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        # Get post-update accuracies\n",
    "        y_prob = y_pred.softmax(dim=1)\n",
    "        task_predictions.append(y_prob)\n",
    "\n",
    "        # Accumulate losses and gradients\n",
    "        task_losses.append(loss)\n",
    "        gradients = torch.autograd.grad(loss, fast_weights.values(), \n",
    "                                        create_graph=create_graph)\n",
    "        named_grads = {name: g for ((name, _), g) in zip(fast_weights.items(), \n",
    "                                                         gradients)}\n",
    "        task_gradients.append(named_grads)\n",
    "\n",
    "    if order == 1:\n",
    "        if train:\n",
    "            sum_task_gradients = {k: torch.stack(\n",
    "                [grad[k] for grad in task_gradients]).mean(dim=0)\n",
    "                                  for k in task_gradients[0].keys()}\n",
    "            hooks = []\n",
    "            for name, param in model.named_parameters():\n",
    "                hooks.append(\n",
    "                    param.register_hook(replace_grad(sum_task_gradients, name))\n",
    "                )\n",
    "\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            # Dummy pass in order to create `loss` variable\n",
    "            # Replace dummy gradients with mean task gradients using hooks\n",
    "            y_pred = model(torch.zeros((k_way, ) + data_shape).to(device, \n",
    "                                                              dtype=torch.double))\n",
    "            loss = loss_fn(y_pred, create_nshot_task_label(k_way, 1).to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            for h in hooks:\n",
    "                h.remove()\n",
    "\n",
    "        return torch.stack(task_losses).mean(), torch.cat(task_predictions)\n",
    "\n",
    "    elif order == 2:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        meta_batch_loss = torch.stack(task_losses).mean()\n",
    "\n",
    "        if train:\n",
    "            meta_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return meta_batch_loss, torch.cat(task_predictions)\n",
    "    else:\n",
    "        raise ValueError('Order must be either 1 or 2.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a few utility functions for evaluating model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T19:57:41.683585Z",
     "iopub.status.busy": "2022-01-19T19:57:41.683288Z",
     "iopub.status.idle": "2022-01-19T19:57:41.695531Z",
     "shell.execute_reply": "2022-01-19T19:57:41.694700Z",
     "shell.execute_reply.started": "2022-01-19T19:57:41.683531Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def calculate_accuracy(y_pred, y):\n",
    "    return torch.eq(y_pred.argmax(dim=-1), y).sum().item() / y_pred.shape[0]\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write the train and evaluate functions which we will then call for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T19:57:41.698110Z",
     "iopub.status.busy": "2022-01-19T19:57:41.697324Z",
     "iopub.status.idle": "2022-01-19T19:57:41.710796Z",
     "shell.execute_reply": "2022-01-19T19:57:41.709810Z",
     "shell.execute_reply.started": "2022-01-19T19:57:41.698049Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model: nn.Module, \n",
    "            taskloader: data.DataLoader, \n",
    "            optimizer: optim.Optimizer, \n",
    "            loss_fn: Callable, \n",
    "            prepare_batch: Callable, \n",
    "            **kwargs):\n",
    "    seen = 0\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "\n",
    "    for batch_index, batch in enumerate(taskloader):\n",
    "        x, y = prepare_batch(batch)\n",
    "\n",
    "        loss, y_pred = meta_gradient_step(\n",
    "            model,\n",
    "            optimizer,\n",
    "            loss_fn,\n",
    "            x,\n",
    "            y,\n",
    "            **kwargs)\n",
    "\n",
    "        seen += y_pred.shape[0]\n",
    "\n",
    "        total_loss += loss.item() * y_pred.shape[0]\n",
    "        total_acc += calculate_accuracy(y_pred, y) * y_pred.shape[0]\n",
    "\n",
    "    total_loss = total_loss/seen\n",
    "    total_acc = total_acc/seen\n",
    "    return total_loss, total_acc\n",
    "\n",
    "def evaluate(model: nn.Module, \n",
    "            taskloader: data.DataLoader, \n",
    "            optimizer: optim.Optimizer, \n",
    "            loss_fn: Callable, \n",
    "            prepare_batch: Callable, \n",
    "            **kwargs):\n",
    "    seen = 0\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    for batch_index, batch in enumerate(taskloader):\n",
    "        x, y = prepare_batch(batch)\n",
    "\n",
    "        loss, y_pred = meta_gradient_step(\n",
    "            model,\n",
    "            optimizer,\n",
    "            loss_fn,\n",
    "            x,\n",
    "            y,\n",
    "            **kwargs)\n",
    "\n",
    "        seen += y_pred.shape[0]\n",
    "\n",
    "        total_loss += loss.item()* y_pred.shape[0]\n",
    "        total_acc += calculate_accuracy(y_pred, y) * y_pred.shape[0]\n",
    "\n",
    "    total_loss = total_loss/seen\n",
    "    total_acc = total_acc/seen\n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now construct the dataloader, set the hyperparameters for the algorithm, and for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T19:57:41.714429Z",
     "iopub.status.busy": "2022-01-19T19:57:41.713773Z",
     "iopub.status.idle": "2022-01-19T19:57:43.713233Z",
     "shell.execute_reply": "2022-01-19T19:57:43.712358Z",
     "shell.execute_reply.started": "2022-01-19T19:57:41.714387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 33,026 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "### 3-shot, 2-way classification ###\n",
    "n_shot = 3\n",
    "k_way = 2\n",
    "q_queries = n_shot\n",
    "meta_batch_size = 2\n",
    "\n",
    "INPUT_DIM = len(select_species)\n",
    "OUTPUT_DIM = k_way\n",
    "LAYER_SIZE = 128\n",
    "inner_lr = 0.01\n",
    "meta_lr = 0.001\n",
    "epoch_len = 800\n",
    "epochs = 50\n",
    "first_order_epochs = int(0.2*epochs)\n",
    "second_order_epochs = epochs - first_order_epochs\n",
    "eval_batches = 80\n",
    "inner_train_steps = 5\n",
    "inner_val_steps = 5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "meta_model = MLP(INPUT_DIM, OUTPUT_DIM, LAYER_SIZE).to(device, dtype=torch.double)\n",
    "meta_optimizer = optim.Adam(meta_model.parameters(), lr=meta_lr)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "print(f'The model has {count_parameters(meta_model):,} trainable parameters')\n",
    "\n",
    "train_dataset = SpeciesAbundanceDataset(train_df, select_species, 'disease_id')\n",
    "test_dataset = SpeciesAbundanceDataset(test_df, select_species, 'disease_id')\n",
    "\n",
    "train_taskloader = data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_sampler=NShotTaskSampler(train_dataset, epoch_len, n=n_shot, \n",
    "                                   k=k_way, q=q_queries,\n",
    "                                   num_tasks=meta_batch_size),\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "test_taskloader = data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_sampler=NShotTaskSampler(test_dataset, eval_batches, n=n_shot, \n",
    "                                   k=k_way, q=q_queries,\n",
    "                                   num_tasks=meta_batch_size),\n",
    "    num_workers=1\n",
    ")\n",
    "\n",
    "\n",
    "train_kwargs = {'n_shot':n_shot, 'k_way': k_way, 'q_queries': q_queries,\n",
    "                'inner_train_steps': inner_train_steps, 'inner_lr': inner_lr,\n",
    "                'order': 1, 'train': True, 'device': device} \n",
    "\n",
    "test_kwargs = {'n_shot':n_shot, 'k_way': k_way, 'q_queries': q_queries,\n",
    "                'inner_train_steps': inner_val_steps, 'inner_lr': inner_lr,\n",
    "                'order': 1, 'train': True, 'device': device}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T19:57:43.714939Z",
     "iopub.status.busy": "2022-01-19T19:57:43.714672Z",
     "iopub.status.idle": "2022-01-19T20:48:56.318421Z",
     "shell.execute_reply": "2022-01-19T20:48:56.317540Z",
     "shell.execute_reply.started": "2022-01-19T19:57:43.714901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2ec724ec2e42f09ba1c56a9f0b72a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01/10 | Epoch Time: 0m 57s\n",
      "\tTrain Loss: 0.423 | Train Acc: 82.57%\n",
      "\t Val. Loss: 0.544| Best Val. Loss:0.544 |  Val. Acc: 74.69%\n",
      "Epoch: 02/10 | Epoch Time: 0m 58s\n",
      "\tTrain Loss: 0.242 | Train Acc: 90.56%\n",
      "\t Val. Loss: 0.485| Best Val. Loss:0.485 |  Val. Acc: 78.23%\n",
      "Epoch: 03/10 | Epoch Time: 0m 58s\n",
      "\tTrain Loss: 0.233 | Train Acc: 91.99%\n",
      "\t Val. Loss: 0.390| Best Val. Loss:0.390 |  Val. Acc: 83.12%\n",
      "Epoch: 04/10 | Epoch Time: 0m 58s\n",
      "\tTrain Loss: 0.197 | Train Acc: 92.93%\n",
      "\t Val. Loss: 0.353| Best Val. Loss:0.353 |  Val. Acc: 85.62%\n",
      "Epoch: 05/10 | Epoch Time: 0m 58s\n",
      "\tTrain Loss: 0.170 | Train Acc: 94.20%\n",
      "\t Val. Loss: 0.378| Best Val. Loss:0.353 |  Val. Acc: 84.17%\n",
      "Epoch: 06/10 | Epoch Time: 0m 58s\n",
      "\tTrain Loss: 0.186 | Train Acc: 93.96%\n",
      "\t Val. Loss: 0.354| Best Val. Loss:0.353 |  Val. Acc: 85.83%\n",
      "Epoch: 07/10 | Epoch Time: 0m 59s\n",
      "\tTrain Loss: 0.156 | Train Acc: 94.80%\n",
      "\t Val. Loss: 0.326| Best Val. Loss:0.326 |  Val. Acc: 86.46%\n",
      "Epoch: 08/10 | Epoch Time: 0m 58s\n",
      "\tTrain Loss: 0.172 | Train Acc: 94.59%\n",
      "\t Val. Loss: 0.310| Best Val. Loss:0.310 |  Val. Acc: 87.08%\n",
      "Epoch: 09/10 | Epoch Time: 0m 58s\n",
      "\tTrain Loss: 0.169 | Train Acc: 95.12%\n",
      "\t Val. Loss: 0.334| Best Val. Loss:0.310 |  Val. Acc: 85.94%\n",
      "Epoch: 10/10 | Epoch Time: 0m 58s\n",
      "\tTrain Loss: 0.145 | Train Acc: 95.29%\n",
      "\t Val. Loss: 0.347| Best Val. Loss:0.310 |  Val. Acc: 85.73%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511f2d68f46342378f85b7f5a330a5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.166 | Train Acc: 94.77%\n",
      "\t Val. Loss: 0.378| Best Val. Loss: 0.310 |  Val. Acc: 84.17%\n",
      "Epoch: 02/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.140 | Train Acc: 95.97%\n",
      "\t Val. Loss: 0.341| Best Val. Loss: 0.310 |  Val. Acc: 85.42%\n",
      "Epoch: 03/40 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.126 | Train Acc: 96.33%\n",
      "\t Val. Loss: 0.315| Best Val. Loss: 0.310 |  Val. Acc: 88.12%\n",
      "Epoch: 04/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.124 | Train Acc: 95.98%\n",
      "\t Val. Loss: 0.322| Best Val. Loss: 0.310 |  Val. Acc: 86.15%\n",
      "Epoch: 05/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.119 | Train Acc: 96.97%\n",
      "\t Val. Loss: 0.283| Best Val. Loss: 0.283 |  Val. Acc: 87.08%\n",
      "Epoch: 06/40 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.135 | Train Acc: 96.12%\n",
      "\t Val. Loss: 0.322| Best Val. Loss: 0.283 |  Val. Acc: 85.42%\n",
      "Epoch: 07/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.106 | Train Acc: 97.06%\n",
      "\t Val. Loss: 0.299| Best Val. Loss: 0.283 |  Val. Acc: 86.88%\n",
      "Epoch: 08/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.122 | Train Acc: 96.42%\n",
      "\t Val. Loss: 0.292| Best Val. Loss: 0.283 |  Val. Acc: 87.81%\n",
      "Epoch: 09/40 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.120 | Train Acc: 96.86%\n",
      "\t Val. Loss: 0.263| Best Val. Loss: 0.263 |  Val. Acc: 89.27%\n",
      "Epoch: 10/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.109 | Train Acc: 97.21%\n",
      "\t Val. Loss: 0.239| Best Val. Loss: 0.239 |  Val. Acc: 89.17%\n",
      "Epoch: 11/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.106 | Train Acc: 96.90%\n",
      "\t Val. Loss: 0.312| Best Val. Loss: 0.239 |  Val. Acc: 87.19%\n",
      "Epoch: 12/40 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.110 | Train Acc: 96.89%\n",
      "\t Val. Loss: 0.243| Best Val. Loss: 0.239 |  Val. Acc: 89.48%\n",
      "Epoch: 13/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.101 | Train Acc: 97.10%\n",
      "\t Val. Loss: 0.269| Best Val. Loss: 0.239 |  Val. Acc: 89.06%\n",
      "Epoch: 14/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.108 | Train Acc: 97.03%\n",
      "\t Val. Loss: 0.256| Best Val. Loss: 0.239 |  Val. Acc: 88.54%\n",
      "Epoch: 15/40 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.093 | Train Acc: 97.42%\n",
      "\t Val. Loss: 0.280| Best Val. Loss: 0.239 |  Val. Acc: 89.38%\n",
      "Epoch: 16/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.097 | Train Acc: 97.19%\n",
      "\t Val. Loss: 0.217| Best Val. Loss: 0.217 |  Val. Acc: 90.83%\n",
      "Epoch: 17/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.105 | Train Acc: 97.17%\n",
      "\t Val. Loss: 0.247| Best Val. Loss: 0.217 |  Val. Acc: 91.46%\n",
      "Epoch: 18/40 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.092 | Train Acc: 97.47%\n",
      "\t Val. Loss: 0.218| Best Val. Loss: 0.217 |  Val. Acc: 91.04%\n",
      "Epoch: 19/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.098 | Train Acc: 97.34%\n",
      "\t Val. Loss: 0.198| Best Val. Loss: 0.198 |  Val. Acc: 91.98%\n",
      "Epoch: 20/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.108 | Train Acc: 97.25%\n",
      "\t Val. Loss: 0.202| Best Val. Loss: 0.198 |  Val. Acc: 92.08%\n",
      "Epoch: 21/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.088 | Train Acc: 97.77%\n",
      "\t Val. Loss: 0.201| Best Val. Loss: 0.198 |  Val. Acc: 92.40%\n",
      "Epoch: 22/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.100 | Train Acc: 97.14%\n",
      "\t Val. Loss: 0.266| Best Val. Loss: 0.198 |  Val. Acc: 88.02%\n",
      "Epoch: 23/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.096 | Train Acc: 97.35%\n",
      "\t Val. Loss: 0.197| Best Val. Loss: 0.197 |  Val. Acc: 92.92%\n",
      "Epoch: 24/40 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.106 | Train Acc: 96.91%\n",
      "\t Val. Loss: 0.218| Best Val. Loss: 0.197 |  Val. Acc: 91.25%\n",
      "Epoch: 25/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.094 | Train Acc: 97.02%\n",
      "\t Val. Loss: 0.198| Best Val. Loss: 0.197 |  Val. Acc: 91.77%\n",
      "Epoch: 26/40 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.087 | Train Acc: 97.60%\n",
      "\t Val. Loss: 0.198| Best Val. Loss: 0.197 |  Val. Acc: 92.29%\n",
      "Epoch: 27/40 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.091 | Train Acc: 97.26%\n",
      "\t Val. Loss: 0.237| Best Val. Loss: 0.197 |  Val. Acc: 89.48%\n",
      "Epoch: 28/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.096 | Train Acc: 97.10%\n",
      "\t Val. Loss: 0.209| Best Val. Loss: 0.197 |  Val. Acc: 90.73%\n",
      "Epoch: 29/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.080 | Train Acc: 97.96%\n",
      "\t Val. Loss: 0.223| Best Val. Loss: 0.197 |  Val. Acc: 91.77%\n",
      "Epoch: 30/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.092 | Train Acc: 97.45%\n",
      "\t Val. Loss: 0.188| Best Val. Loss: 0.188 |  Val. Acc: 93.44%\n",
      "Epoch: 31/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.092 | Train Acc: 97.17%\n",
      "\t Val. Loss: 0.220| Best Val. Loss: 0.188 |  Val. Acc: 91.35%\n",
      "Epoch: 32/40 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.092 | Train Acc: 97.38%\n",
      "\t Val. Loss: 0.169| Best Val. Loss: 0.169 |  Val. Acc: 94.06%\n",
      "Epoch: 33/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.075 | Train Acc: 97.61%\n",
      "\t Val. Loss: 0.203| Best Val. Loss: 0.169 |  Val. Acc: 91.88%\n",
      "Epoch: 34/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.079 | Train Acc: 97.79%\n",
      "\t Val. Loss: 0.172| Best Val. Loss: 0.169 |  Val. Acc: 93.54%\n",
      "Epoch: 35/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.082 | Train Acc: 97.41%\n",
      "\t Val. Loss: 0.184| Best Val. Loss: 0.169 |  Val. Acc: 93.85%\n",
      "Epoch: 36/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.087 | Train Acc: 97.31%\n",
      "\t Val. Loss: 0.190| Best Val. Loss: 0.169 |  Val. Acc: 92.60%\n",
      "Epoch: 37/40 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.087 | Train Acc: 97.36%\n",
      "\t Val. Loss: 0.229| Best Val. Loss: 0.169 |  Val. Acc: 91.77%\n",
      "Epoch: 38/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.087 | Train Acc: 97.60%\n",
      "\t Val. Loss: 0.181| Best Val. Loss: 0.169 |  Val. Acc: 93.54%\n",
      "Epoch: 39/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.086 | Train Acc: 97.65%\n",
      "\t Val. Loss: 0.185| Best Val. Loss: 0.169 |  Val. Acc: 94.17%\n",
      "Epoch: 40/40 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.093 | Train Acc: 97.42%\n",
      "\t Val. Loss: 0.173| Best Val. Loss: 0.169 |  Val. Acc: 93.75%\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "for epoch in trange(first_order_epochs):\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    train_loss, train_acc = train(meta_model, \n",
    "                                  train_taskloader, \n",
    "                                  meta_optimizer, \n",
    "                                  loss_fn, \n",
    "                                  prepare_meta_batch(n_shot, k_way, q_queries, \n",
    "                                                    meta_batch_size),\n",
    "                                  **train_kwargs\n",
    "                                 )\n",
    "    \n",
    "    valid_loss, valid_acc = evaluate(meta_model, \n",
    "                                  test_taskloader, \n",
    "                                  meta_optimizer, \n",
    "                                  loss_fn, \n",
    "                                  prepare_meta_batch(n_shot, k_way, q_queries, \n",
    "                                                    meta_batch_size),\n",
    "                                  **test_kwargs\n",
    "                                 )\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(meta_model.state_dict(),'MLP-classifier.pt')\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}/{first_order_epochs} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}| Best Val. Loss:'\n",
    "          f'{best_valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "for epoch in trange(second_order_epochs):\n",
    "    start_time = time.monotonic()\n",
    "\n",
    "    train_kwargs['order'] = 2\n",
    "    train_loss, train_acc = train(meta_model, \n",
    "                                  train_taskloader, \n",
    "                                  meta_optimizer, \n",
    "                                  loss_fn, \n",
    "                                  prepare_meta_batch(n_shot, k_way, q_queries, \n",
    "                                                    meta_batch_size),\n",
    "                                  **train_kwargs\n",
    "                                 )\n",
    "\n",
    "    test_kwargs['order'] = 2    \n",
    "    valid_loss, valid_acc = evaluate(meta_model, \n",
    "                                  test_taskloader, \n",
    "                                  meta_optimizer, \n",
    "                                  loss_fn, \n",
    "                                  prepare_meta_batch(n_shot, k_way, q_queries, \n",
    "                                                    meta_batch_size),\n",
    "                                  **test_kwargs\n",
    "                                 )\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(meta_model.state_dict(),'MLP-classifier.pt')\n",
    "\n",
    "    end_time = time.monotonic()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}/{second_order_epochs} | Epoch Time: '\n",
    "          f'{epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}| Best Val. Loss: '\n",
    "          f'{best_valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing functions for calculating different performance metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T20:48:56.320872Z",
     "iopub.status.busy": "2022-01-19T20:48:56.320046Z",
     "iopub.status.idle": "2022-01-19T20:48:56.333185Z",
     "shell.execute_reply": "2022-01-19T20:48:56.332469Z",
     "shell.execute_reply.started": "2022-01-19T20:48:56.320809Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(y_pred, y):\n",
    "    ### calculate confusion matrix, f1_score, roc_auc score\n",
    "    pred_labels = y_pred.argmax(dim=-1).cpu()\n",
    "    true_labels = y.cpu()\n",
    "    cm = metrics.confusion_matrix(true_labels, pred_labels)\n",
    "    f1_score = metrics.f1_score(true_labels, pred_labels)\n",
    "    roc_auc_score = metrics.roc_auc_score(true_labels, pred_labels)\n",
    "    return cm, f1_score, roc_auc_score\n",
    "\n",
    "def evaluate_metrics(model: nn.Module, \n",
    "            taskloader: data.DataLoader, \n",
    "            optimizer: optim.Optimizer, \n",
    "            loss_fn: Callable, \n",
    "            prepare_batch: Callable, \n",
    "            **kwargs):\n",
    "    seen = 0\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    \n",
    "    for batch_index, batch in enumerate(taskloader):\n",
    "        x, y = prepare_batch(batch)\n",
    "\n",
    "        loss, y_pred = meta_gradient_step(\n",
    "            model,\n",
    "            optimizer,\n",
    "            loss_fn,\n",
    "            x,\n",
    "            y,\n",
    "            **kwargs)\n",
    "\n",
    "        seen += y_pred.shape[0]\n",
    "\n",
    "        total_loss += loss.item()* y_pred.shape[0]\n",
    "        total_acc += calculate_accuracy(y_pred, y) * y_pred.shape[0]\n",
    "        if batch_index == 0:\n",
    "            total_cm, avg_f1_score, avg_roc_auc_score = calculate_metrics(y_pred,\n",
    "                                                                             y)\n",
    "        else:\n",
    "            cm, f1_score, roc_auc_score = calculate_metrics(y_pred, y)\n",
    "            total_cm += cm\n",
    "            avg_f1_score += f1_score\n",
    "            avg_roc_auc_score += roc_auc_score\n",
    "        \n",
    "    total_loss = total_loss/seen\n",
    "    total_acc = total_acc/seen\n",
    "    avg_f1_score = avg_f1_score/batch_index\n",
    "    avg_roc_auc_score = avg_roc_auc_score/batch_index\n",
    "    return total_loss, total_acc, total_cm, avg_f1_score, avg_roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model parameters with the best validation loss, and calculating different performance metrics for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-19T20:48:56.336533Z",
     "iopub.status.busy": "2022-01-19T20:48:56.336296Z",
     "iopub.status.idle": "2022-01-19T20:49:00.009010Z",
     "shell.execute_reply": "2022-01-19T20:49:00.008263Z",
     "shell.execute_reply.started": "2022-01-19T20:48:56.336506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val. Loss: 0.173| Best Val. Loss: 0.169 |  Val. Acc: 93.75%\n",
      "Val. F1 score: 0.9700164814088859 | Val. ROC AUC score: 0.9694092827004219\n",
      "Confusion matrix:\n",
      "[[457  23]\n",
      " [ 18 462]]\n"
     ]
    }
   ],
   "source": [
    "meta_model.load_state_dict(torch.load('MLP-classifier.pt'))\n",
    "\n",
    "test_kwargs['order'] = 2    \n",
    "eval_loss, eval_acc, eval_cm, eval_f1, eval_roc_auc = evaluate_metrics(\n",
    "                                meta_model, \n",
    "                                test_taskloader, \n",
    "                                meta_optimizer, \n",
    "                                loss_fn, \n",
    "                                prepare_meta_batch(n_shot, k_way, q_queries, \n",
    "                                                            meta_batch_size),\n",
    "                                **test_kwargs)\n",
    "\n",
    "print(f'Val. Loss: {valid_loss:.3f}| Best Val. Loss: '\n",
    "      f'{best_valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "print(f'Val. F1 score: {eval_f1} | Val. ROC AUC score: {eval_roc_auc}')\n",
    "print(f'Confusion matrix:')\n",
    "print(eval_cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
